<html lang="en-GB">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation</title>
    <meta name="description"
        content="Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="Blockwise Flow Matching" property="og:title">
    <meta content="Improving Flow Matching Models For Efficient High-Quality Generation" property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@koparkrea">
    <meta name="twitter:title"
        content="Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation">
    <meta name="twitter:description"
        content="Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation.">
    <meta name="twitter:image" content="assets/figures/main_figure.png">

    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css" />
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <!-- <script src="assets/scripts/navbar.js"></script> Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme Example: Change the background colour dark and change the following div "blog-title" into "blog-title white". -->
    <div class="container blog" id="first-content" style="background-color: #f6f6f6;">
        <!-- style="background:radial-gradient(circle at 50% 50%, #9BD4E5 0%, #5FA9D4 80%),linear-gradient(120deg, #A8D5A2 0%, #D6EAC7 50%, transparent 100%),linear-gradient(200deg, #F8C7D8 15%, #E1C4F4 40%, transparent 80%),linear-gradient(45deg, #FFF4D6 0%, #FCEABB 30%, transparent 70%);background-blend-mode: overlay, lighten, screen, normal;"> -->
        <div class="blog-title no-cover">
            <div class="blog-intro">
                <div>
                    <h1 class="title"><b>Blockwise Flow Matching</b>: Improving Flow Matching Models For Efficient
                        High-Quality
                        Generation
                    </h1>
                    <p class="author">
                        <a href="https://dogyunpark.github.io">Dogyun Park</a><sup>1</sup>, Taehoon Lee<sup>2</sup>,
                        Minseok
                        Joo<sup>1</sup>,
                        Hyunwoo J. Kim<sup>2</sup>
                    </p>
                    <p class="author" style="padding-top: 0px;">
                        <sup>1</sup>Korea University, <sup>2</sup>KAIST
                    </p>


                    <img src="assets/figures/main_figure.png" alt="Project figure" style="width: 100%;">
                    <p class="caption"
                        style="text-align: center; width: 100%; margin-left: auto; margin-right: auto; padding-top: 0px;">
                        <b>BFM establishes a
                            substantially improved Pareto frontier over existing
                            Flow Matching methods, achieving 2.1x to 4.9x accelerations in inference complexity at
                            comparable generation performance.</b>
                    </p>


                    <p class="abstract" style="text-align: justify;">
                        <b>TL;DR:</b> We propose <b>Blockwise Flow Matching (BFM)</b>, a novel framework that partitions
                        the
                        generative trajectory into multiple temporal segments, each modeled by smaller but specialized
                        velocity blocks. This blockwise design enables each block to specialize effectively in its
                        designated interval, improving inference efficiency and sample quality.
                    </p>

                </div>

                <!-- Using FontAwesome Free -->
                <div class="info" style="padding-top: 20px;">
                    <div>
                        <a href="https://arxiv.org/pdf/2510.21167" class="button icon"
                            style="background-color: rgba(255, 255, 255, 0.2)"> Paper <i
                                class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp;
                        <a href="https://github.com/mlvlab/Blockwise-Flow-Matching" class="button icon"
                            style="background-color: rgba(255, 255, 255, 0.2)">Code <i class="fa-solid fa-code"></i></a>
                        &nbsp;&nbsp;
                        <!--            <a href="https://www.microsoft.com/en-gb/microsoft-365/powerpoint" class="button icon"-->
                        <!--               style="background-color: rgba(255, 255, 255, 0.2);">Slides <i class="fa-regular fa-file-powerpoint"></i></a>-->
                        <!--            &nbsp;&nbsp;-->
                        <!--            <a href="" class="button icon"-->
                        <!--               style="background-color: rgba(255, 255, 255, 0.2)">Demo (coming soon)<i class="fa-solid fa-laptop-code"></i></a>-->
                    </div>
                </div>
                <div class="info">
                    <p>NeurIPS 2025</p>
                </div>
            </div>

            <!-- <div class="blog-cover">
                <img class="foreground" src="assets/figures/clarity.png">
                <img class="background" src="assets/figures/clarity.png">
            </div> -->
        </div>
    </div>

    <div class="container blog main first" id="blog-main" sytle="text-align: justify;">

        <h1>Qualitative Results
            <span style="font-size: 0.5em; font-weight: normal;">
                (Hover over the image to magnify)
            </span>
        </h1>
        <div style="position: relative; width: 100%; margin-top: 10px;">
            <img id="imgSource" src="assets/figures/main_qual.png" style="width: 100%; height: auto; display: block;"
                alt="Qualitative Results">
            <div id="magnifier">
                <img id="magImg" alt="" aria-hidden="true">
            </div>
        </div>


        <p class="caption" style="width: 100%; text-align: center; margin-left: auto; margin-right: auto;">
            <b>Generated samples from BFM-XL$_{\text{SF}}$ on ImageNet 256$\times$256 with classifier-free guidance ($w
                =
                4.0$).</b>
        </p>

        <script>
            const el = (s, p = document) => p.querySelector(s);

            const elImg = el("#imgSource");
            const elMag = el("#magnifier");
            const elMagImg = el("#magImg");
            const radius = elMag.offsetWidth / 2;
            const zoom = 2.5; // tune

            let rect = null;
            let lastEvt = null;
            let raf = 0;

            function syncRectAndSource() {
                rect = elImg.getBoundingClientRect();
                // mirror the rendered size so our math is in CSS px
                elMagImg.style.width = rect.width + "px";
                elMagImg.style.height = rect.height + "px";
                // use the actually chosen source (handles srcset)
                const src = elImg.currentSrc || elImg.src;
                if (elMagImg.src !== src) elMagImg.src = src;
            }

            function tick() {
                raf = 0;
                if (!lastEvt || !rect) return;

                const cx = lastEvt.clientX, cy = lastEvt.clientY;
                const x = cx - rect.left;
                const y = cy - rect.top;

                // skip work if pointer is outside the image
                if (x < 0 || y < 0 || x > rect.width || y > rect.height) return;

                // move the lens (compositor only)
                elMag.style.transform = `translate3d(${cx}px, ${cy}px, 0)`;

                // bring (x,y) under the cursor to the lens center using only transforms
                // order matters: rightmost applied first
                elMagImg.style.transform =
                    `translate3d(${radius}px, ${radius}px, 0) ` + // final offset (not scaled)
                    `scale(${zoom}) ` +                            // scale around (0,0)
                    `translate3d(${-x}px, ${-y}px, 0)`;            // pre-scale translation
            }

            function onMove(e) {
                lastEvt = e;
                if (!raf) raf = requestAnimationFrame(tick);
            }

            function onResizeOrScroll() {
                // throttle to one recalc/frame
                cancelAnimationFrame(raf);
                raf = requestAnimationFrame(() => {
                    syncRectAndSource();
                    tick();
                });
            }

            // events
            elImg.addEventListener("pointermove", onMove, { passive: true });
            elImg.addEventListener("pointerdown", onMove, { passive: true });
            window.addEventListener("resize", onResizeOrScroll, { passive: true });
            window.addEventListener("scroll", onResizeOrScroll, { passive: true });

            // init (after image has pixels)
            if (elImg.complete) syncRectAndSource();
            else elImg.addEventListener("load", syncRectAndSource);
        </script>
    </div>

    <div class="container blog main">
        <h1>
            Abstract
        </h1>
        <p class='text' style="text-align: justify;">
            Recently, Flow Matching models have pushed the boundaries of high-fidelity data generation across a wide
            range of domains. It typically employs a single large network to learn the entire generative trajectory from
            noise to data. Despite their effectiveness, this design struggles to capture distinct signal characteristics
            across timesteps simultaneously and incurs substantial inference costs due to the iterative evaluation of
            the entire model. To address these limitations, we propose Blockwise Flow Matching (BFM), a novel framework
            that partitions the generative trajectory into multiple temporal segments, each modeled by smaller but
            specialized velocity blocks. This blockwise design enables each block to specialize effectively in its
            designated interval, improving inference efficiency and sample quality. To further enhance generation
            fidelity, we introduce a Semantic Feature Guidance module that explicitly conditions velocity blocks on
            semantically rich features aligned with pretrained representations.
            Additionally, we propose a lightweight Feature Residual Approximation strategy that preserves semantic
            quality while significantly reducing inference cost.
            Extensive experiments on ImageNet 256$\times$256 demonstrate that BFM establishes a substantially improved
            Pareto frontier over existing Flow Matching methods, achieving 2.1$\times$ to 4.9$\times$ accelerations in
            inference complexity at comparable generation performance.
        </p>
    </div>

    <div class="container blog main">
        <h1>
            Method Overview
        </h1>
        <h2>
            Blockwise Flow Matching with Semantic Feature Guidance
        </h2>
    </div>
    <div class="container blog main gray">
        <div class="content">
            <img src="assets/figures/method1.png" style="width: 100%">
            <p class="caption" style="width: 100%; text-align: left; margin-left: auto; margin-right: auto;">
                <b>Figure 1:</b> Blockwise Flow Matching partitions the flow trajectory
                into M segments, each modeled by a specialized velocity block $v^{(m)}_θ$ . Semantic Feature
                Guidance
                enhances the velocity block by explicitly conditioning features ft from the feature alignment
                network $f_φ$.
            </p>
        </div>
    </div>

    <div class="container blog main">
        <p class="text" style="text-align: justify;">
            Standard Flow Matching (FM) models train a single large network to model the entire trajectory from noise to
            data, which leads to inefficiency and suboptimal performance due to the conflicting spectral characteristics
            at different timesteps and high inference cost. To overcome these limitations, <b>Blockwise Flow
                Matching (BFM)</b>
            divides the overall trajectory into multiple temporal segments, each modeled by a smaller, specialized
            velocity block. This blockwise design allows each block to focus on specific interval dynamics, reducing
            computation and improving representation efficiency, as only the relevant block is evaluated at each
            timestep.
        </p>

        <p class="text" style="text-align: justify;">
            Moreover, we propose <b>Semantic Feature Guidance (SemFeat)</b> that introduces a shared feature alignment
            network $f_φ$ that
            extracts robust semantic features $f_t$ from intermediate noisy states and aligns them with clean image
            embeddings from
            pretrained visual
            encoders like DINOv2. These semantic features are then used to condition each velocity block. The final
            training objective combines
            the
            blockwise flow matching loss and the semantic alignment loss. Our SemFeat approach
            <span class="hover-container">enriches
                the velocity blocks' representations and improves the overall generative quality
                <img src="assets/figures/ablation.png" class="hover-image" style="width: 100%; height: auto;">
            </span>, enabling semantically consistent generation across all temporal segments.
        </p>
    </div>

    <div class="container blog main">
        <h2>
            Feature Residual Approximation
        </h2>
    </div>

    <div class="container blog main gray">
        <div class="content">
            <img src="assets/figures/method2.png" style="width: 100%">
            <p class="caption" style="width: 100%; text-align: left; margin-left: auto; margin-right: auto;">
                <b>Figure 2:</b> <b>(left)</b>: After training velocity models and $f_\phi$, we freeze them and train
                the
                Feature Residual Network (FRN) to efficiently approximate $f_t$ by a residual connection.
                <b>(right)</b>:
                During inference, samples can be efficiently generated by evaluating $f_\phi$ once per segment, reducing
                inference complexity.
            </p>
        </div>
    </div>

    <div class="container blog main">
        <p class="text" style="text-align: justify;">
            While the feature alignment network $f_φ$ enhances generation quality through rich semantic guidance,
            evaluating it at every timestep incurs substantial computational cost. To preserve efficiency, the Feature
            Residual Approximation (FRA) introduces a lightweight Feature Residual Network (FRN), $f_η$, trained to
            approximate the semantic features produced by $f_φ$ within each temporal segment. After freezing the
            pretrained alignment and velocity networks, FRN learns to predict the residual evolution of semantic
            features from the segment’s start feature $f_{t_{m-1}}$, scaled by a normalized offset $b_m(t)$. This design
            enables feature updates to be computed incrementally and efficiently, requiring $f_φ$ to run only once per
            segment. The FRN is trained via a feature regression loss to match the frozen alignment features.
            Empirically, this residual approximation retains over
            <span class="hover-container">98% of the semantic fidelity of
                BFM-S$_{\text{SF}}$ while cutting inference complexity by 41%
                <img src="assets/figures/ablation.png" class="hover-image" style="width: 100%; height: auto;">
            </span>, offering a strong trade-off between quality and efficiency.
        </p>
    </div>

    <div class="container blog main">
        <h1>Results</h1>
        <h2>Comparison with start-of-the-art methods</h2>
        <p class="text" style="text-align: justify">
            Table 1 compares our Blockwise Flow Matching (BFM) framework with recent state-of-the-art
            generative models. The proposed BFM-XL$_{\text{SF}}$, enhanced with Semantic Feature Guidance, achieves the
            best overall performance, attaining an FID of 1.75 and surpassing models such as FiTv2, DiT, SiT, and REPA.
            With the addition of the Feature Residual Approximation (FRA) module,
            BFM-XL$_{\text{SF-RA}}$ further enhances efficiency—achieving a 67% FLOP reduction (roughly 3× faster
            inference) while maintaining high fidelity.
        </p>

        <div>
            <figure style="text-align: center; flex: 0.79;">
                <img src="assets/figures/main_table.png" style="max-width: 100%; height: auto;">
                <p class="caption" style="width: 100%; text-align: left; margin-left: auto; margin-right: auto;">
                    <b>Table 1:</b> <b>System-level comparison on ImageNet 256$\times$256</b> class-conditioned
                    generation with
                    classifier-free guidance. FLOPs are averaged across the solver steps and number of samples.↓ and
                    ↑ indicate whether lower or higher values are better, respectively. † use guidance
                    scheduling.
                </p>
            </figure>
        </div>

        <h2>Analysis</h2>
        <p class="text" style="text-align: justify;">
            <b>SemFeat vs. REPA:</b> To evaluate the effectiveness of our proposed Semantic Feature Guidance (SemFeat),
            we compare it with the closely related REPA approach. Unlike REPA, which learns semantic alignment
            implicitly within the velocity model, SemFeat employs an external feature alignment network that explicitly
            conditions semantic features on the generative process. This modular design decouples representation
            learning from generation, leading to more coherent and stable feature representations. A PCA-based analysis
            of semantic features (See Figure 3 below) shows that SemFeat produces temporally consistent and semantically
            meaningful embeddings across noise levels, whereas REPA’s features appear noisier and less stable.
        </p>

        <img src="assets/figures/SemFeat-REPA.png" alt="Ablation" style="width: 90%; margin-top: 10px;">
        <p class="caption" style="width: 90%; text-align: center; margin-left: auto; margin-right: auto;">
            <b>Figure 3:</b> PCA of semantic features from SemFeat and REPA.
        </p>

        <p class="text" style="text-align: justify;">
            <b>Spectral analysis:</b> To examine whether blockwise modeling encourages stage-specific specialization
            during generation, we perform a frequency-domain analysis of the generated images. Using the 2D Fourier
            power spectrum and azimuthal integration, we compare the mean spectral power distributions of real and
            generated images to evaluate how well models reproduce spatial frequency structures. In Figure 4 below,
            BFM-XL$_{\text{SF}}$
            achieves a smaller Fréchet distance (0.049) from the real image
            spectrum than SiT-XL (0.054), demonstrating better alignment with natural image frequencies. This result
            validates our hypothesis that blockwise segmentation encourages distinct blocks to specialize in different
            signal bands, improving spectral fidelity and overall image realism.
        </p>
        <div>
            <figure style="display: flex; gap: 10px; align-items: flex-end;">
                <figure style="text-align: center; flex: 1.2;">
                    <img src="assets/figures/spectral_analysis.png" style="max-width: 80%; height: auto;">
                    <p class="caption" style="width: 80%; text-align: center; margin-left: auto; margin-right: auto;">
                        <b>Figure 4:</b> Fourier power spectrum of real images, SiT-generated images, and BFM.
                    </p>
                </figure>

                <figure style="text-align: center; flex: 1;">
                    <img src="assets/figures/FRA.png" style="max-width: 90%; height: auto;">
                    <p class="caption" style="width: 85%; text-align: center; margin-left: auto; margin-right: auto;">
                        <b>Figure 5:</b> Generated images from BFM without (left) and with (right) Residual
                        Approximation.
                    </p>
                </figure>
            </figure>
        </div>
        <p class="text" style="text-align: justify;">
            <b>Effectiveness of FRA:</b> To evaluate the feature residual approximation (FRA)
            strategy, we compare it to a
            direct feature prediction baseline that estimates semantic features independently without residual
            connections, e.g., $\hat{f}_t=f_\eta(x_t, c)$. The residual
            formulation converges faster and
            <span class="hover-container"> achieves lower feature regression loss
                <img src="assets/figures/training_fra.png" class="hover-image" style="width: 100%; height: auto;">
            </span> $\mathcal{L}_{\text{FRN}}$,
            confirming
            that modeling semantic evolution as residual increments from the segment start point yields more stable
            learning. Qualitative examples in Figure 5 below further show negligible perceptual differences, validating
            FRA as a highly efficient yet faithful feature approximation method.
        </p>

    </div>

    <div class="container blog main">
        <h1>Conclusion</h1>
        <p class="text" style="text-align: justify;">
            In this paper, we introduced <b>Blockwise Flow Matching</b> (BFM), a novel generative framework that
            partitions the generative trajectory into distinct temporal segments, each handled by compact and
            specialized velocity blocks. This design allowed each block to effectively capture the unique signal
            characteristics of its interval, leading to improved generation quality and efficiency. By further
            incorporating our proposed <b>Semantic Feature Guidance</b> and <b>Feature Residual Approximation</b>
            modules, we demonstrated that BFM establishes a significantly improved Pareto-frontier between generation
            performance and inference complexity on ImageNet 256$\times$256: <b>2.1$\times$ and 4.9$\times$
                acceleration</b> in inference complexity compared to the existing state-of-the-art generative methods.
        </p>
    </div>

    <div class="container blog main">
        <h1>Citation</h1>
        <p class="text">
            If you find our work useful in your research, please cite it as:
        </p>
        <pre><code class="plaintext">@article{park2025blockwise,
            title={Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation},
            author={Park, Dogyun and Lee, Taehoon and Joo, Minseok and Kim, Hyunwoo J},
            journal={arXiv preprint arXiv:2510.21167},
            year={2025}
          }</code></pre>
    </div>

    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is loosely built on the <a href="https://shikun.io/projects/clarity">Clarity
                    Template</a>
                and <a href="https://wimmerth.github.io/anyup/">Anyup</a>.
            </p>
        </div>
    </footer>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>
    <script src="assets/scripts/main.js"></script>

</body>

</html>